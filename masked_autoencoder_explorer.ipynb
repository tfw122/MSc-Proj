{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hi\")\n",
    "#from src.utils.builder import build_trainer, build_config\n",
    "from src.utils.utils import *\n",
    "from src.utils.builder import *\n",
    "from src.utils.fileio import *\n",
    "from arguments import args\n",
    "from time import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random \n",
    "import pandas as pd \n",
    "\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from src.datasets.transforms.vision_transforms_utils import UnNormalise\n",
    "\n",
    "# --------------- Import Dataset Builder function -----------------\n",
    "from src.datasets.dataset_zoo.stereo_vision_mim.stereo_datasets_mim.cityscapes_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_mim.stereo_datasets_mim.kitti_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_mim.stereo_datasets_mim.falling_things_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_mim.stereo_datasets_mim.sceneflow_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_mim.stereo_datasets_mim.sintel_stereo_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_mim.stereo_datasets_mim.tartan_air_dataset import *\n",
    "\n",
    "from src.datasets.dataset_zoo.stereo_vision_downstream.stereo_datasets_downstream.kitti_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_downstream.stereo_datasets_downstream.falling_things_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_downstream.stereo_datasets_downstream.sceneflow_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_downstream.stereo_datasets_downstream.sintel_stereo_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_downstream.stereo_datasets_downstream.tartan_air_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_downstream.stereo_datasets_downstream.eth3d_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_downstream.stereo_datasets_downstream.middlebury_dataset import *\n",
    "\n",
    "global LOAD_IMG\n",
    "global TESTING\n",
    "\n",
    "LOAD_IMG= \"dataloader\"\n",
    "TESTING = \"downstream\"\n",
    "print(\"end\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"start\")\n",
    "def get_transforms(config, split):\n",
    "    transforms_name = config.dataset_config.preprocess.name\n",
    "    data_transform_cls = registry.get_preprocessor_class(transforms_name)\n",
    "    data_transforms_obj = data_transform_cls(config, split)\n",
    "    return data_transforms_obj\n",
    "\n",
    "def ToArray(img_t):\n",
    "    img = img_t.detach().to(\"cpu\").numpy()\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "    return img\n",
    "\n",
    "def show_image(image, title=''):\n",
    "    # image is [H, W, 3]\n",
    "    assert image.shape[2] == 3\n",
    "    if TESTING=='downstream':\n",
    "        plt.imshow(image.numpy().astype(np.uint8)) # .numpy().astype(np.uint8)\n",
    "    else:\n",
    "        plt.imshow(image)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "def augmentation_parameters(config):\n",
    "    dataset_config = config.dataset_config\n",
    "    preprocess_config = dataset_config.preprocess.vision_transforms.params\n",
    "    aug_params = {'crop_size': preprocess_config.Resize.size, 'min_scale': preprocess_config.spatial_scale[0], 'max_scale': preprocess_config.spatial_scale[1], 'do_flip': preprocess_config.do_flip, 'yjitter': not preprocess_config.noyjitter}\n",
    "    \n",
    "    if hasattr(preprocess_config, \"saturation_range\") and preprocess_config.saturation_range is not None:\n",
    "        aug_params[\"saturation_range\"] = tuple(preprocess_config.saturation_range)\n",
    "    \n",
    "    if hasattr(preprocess_config, \"img_gamma\") and preprocess_config.img_gamma is not None:\n",
    "        aug_params[\"gamma\"] = preprocess_config.img_gamma\n",
    "    \n",
    "    if hasattr(preprocess_config, \"do_flip\") and preprocess_config.do_flip is not None:\n",
    "        aug_params[\"do_flip\"] = preprocess_config.do_flip\n",
    "    return aug_params\n",
    "\n",
    "\n",
    "def run_one_image(left_img, right_img, model, mask_ratio=None):\n",
    "    # make it a batch-like\n",
    "    left_img = left_img.unsqueeze(dim=0)\n",
    "    right_img = right_img.unsqueeze(dim=0)\n",
    "    #x = torch.einsum('nhwc->nchw', x)\n",
    "\n",
    "    # run MAE\n",
    "    y, mask = model(left_img, right_img, mask_ratio) #, mask_ratio=0.75)\n",
    "    \n",
    "    left_recon= y[0]\n",
    "    right_recon = y[1]\n",
    "    left_mask = mask[0]\n",
    "    right_mask = mask[1]\n",
    "    \n",
    "    left_y = model.unpatchify(left_recon)\n",
    "    right_y = model.unpatchify(right_recon)\n",
    "    print(left_y.max(), left_y.min(), right_y.max(), right_y.min())\n",
    "    left_y = torch.einsum('nchw->nhwc', left_y).detach().cpu()\n",
    "    right_y = torch.einsum('nchw->nhwc', right_y).detach().cpu()\n",
    "\n",
    "    # visualize the mask\n",
    "    left_mask = left_mask.detach()\n",
    "    left_mask = left_mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]*model.patch_embed.patch_size[1]*3)  # (N, H*W, p*p*3)\n",
    "    left_mask = model.unpatchify(left_mask)  # 1 is removing, 0 is keeping\n",
    "    left_mask = torch.einsum('nchw->nhwc', left_mask).detach().cpu()\n",
    "    \n",
    "    right_mask = right_mask.detach()\n",
    "    right_mask = right_mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]*model.patch_embed.patch_size[1]*3)  # (N, H*W, p*p*3)\n",
    "    right_mask = model.unpatchify(right_mask)  # 1 is removing, 0 is keeping\n",
    "    right_mask = torch.einsum('nchw->nhwc', right_mask).detach().cpu()\n",
    "    \n",
    "    left_img = torch.einsum('nchw->nhwc', left_img)\n",
    "    right_img = torch.einsum('nchw->nhwc', right_img)\n",
    "\n",
    "    print(\"middle\")\n",
    "\n",
    "    # masked image\n",
    "    left_im_masked = left_img * (1 - left_mask)\n",
    "    right_im_masked = right_img * (1 - right_mask)\n",
    "    \n",
    "    # MAE reconstruction pasted with visible patches\n",
    "    left_im_paste = left_img * (1 - left_mask) + left_y * left_mask\n",
    "    right_im_paste = right_img * (1 - right_mask) + right_y * right_mask\n",
    "\n",
    "    # make the plt figure larger\n",
    "    plt.rcParams['figure.figsize'] = [24, 48]\n",
    "\n",
    "    plt.subplot(1, 6, 1)\n",
    "    show_image(left_img[0], \"Left original\")\n",
    "    \n",
    "    plt.subplot(1, 6, 2)\n",
    "    show_image(right_img[0], \"Right original\")\n",
    "\n",
    "    plt.subplot(1, 6, 3)\n",
    "    show_image(left_im_masked[0], \"Left masked\")\n",
    "    \n",
    "    plt.subplot(1, 6, 4)\n",
    "    show_image(right_im_masked[0], \" Right masked\")\n",
    "\n",
    "    plt.subplot(1, 6, 5)\n",
    "    show_image(left_y[0], \"Left reconstruction\")\n",
    "    \n",
    "    plt.subplot(1, 6, 6)\n",
    "    show_image(right_y[0], \"Right reconstruction\")\n",
    "\n",
    "    #plt.subplot(1, 8, 7)\n",
    "    #show_image(left_im_paste[0], \"reconstruction + visible\")\n",
    "    \n",
    "    #plt.subplot(1, 8, 8)\n",
    "    #show_image(right_im_paste[0], \"reconstruction + visible\")\n",
    "\n",
    "    plt.show()\n",
    "    print(\"end\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    default_config_path= './configs/default.yaml'\n",
    "    model_config_path='./configs/models/masked_image.yaml'\n",
    "    if TESTING=='downstream':\n",
    "        dataset_config_path='./configs/datasets/stereo_downstream.yaml'\n",
    "    else:\n",
    "        dataset_config_path='./configs/datasets/stereo_mim.yaml'\n",
    "    user_config_path='./configs/user/sample.yaml'\n",
    "    local_rank=None\n",
    "    opts=None\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_imports()\n",
    "\n",
    "config = build_config(args)\n",
    "\n",
    "fileio_client = FileIOClient(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = get_transforms(config, 'train')\n",
    "test_transforms = get_transforms(config, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model weights\n",
    "Ensure you pull the weights from s3 locally in a directory of your choice and load that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model; dataset etc;\n",
    "print(os.getcwd())\n",
    "model = build_model(config, ckpt_path= '../data/tiawarner/downstream4/mae_stereo_mim_perceptual/230824-210638/train_outputs/best-model-epoch=018-val_loss=0.52.ckpt')\n",
    "print('::::::: model loaded with ckpt weights :::::::')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image from a path / Dataloader for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "if LOAD_IMG== \"dataloader\":\n",
    "    if TESTING=='downstream':\n",
    "        aug_params = augmentation_parameters(config)\n",
    "        train_dataset = TartanAirEasy(config = config, aug_params = aug_params, split='train')\n",
    "        val_dataset = TartanAirEasy(config = config, aug_params = aug_params, split='val')\n",
    "        \n",
    "        # left_image, right_image = dataset[i]\n",
    "        sample = train_dataset[idx]\n",
    "        _, left_img, right_img, disp, valid = sample\n",
    "\n",
    "        left_image= ToArray(left_img)\n",
    "        right_image= ToArray(right_img)\n",
    "        disp_map = ToArray(disp)\n",
    "        valid_map = ToArray(valid.unsqueeze(0))\n",
    "\n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4)\n",
    "        fig.suptitle('Downstream Stereo Images')\n",
    "        ax1.imshow(left_image)\n",
    "        ax2.imshow(right_image)\n",
    "        ax3.imshow(disp_map, cmap='jet')\n",
    "        ax4.imshow(valid_map)\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        train_dataset = SceneFlowLoader(config, 'train', train_transforms)\n",
    "        val_dataset = SceneFlowLoader(config, 'val', train_transforms)\n",
    "        sample = val_dataset[idx]\n",
    "        left_img, right_img = sample['left_image'], sample['right_image']\n",
    "\n",
    "        left_image= ToArray(left_img)\n",
    "        right_image= ToArray(right_img)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.suptitle('MIM Stereo Images')\n",
    "        ax1.imshow(left_image)\n",
    "        ax1.axis('off')\n",
    "        ax2.imshow(right_image)\n",
    "        ax2.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    img_path_left = \"\"\n",
    "    # \"/data/stereo_data/middlebury/MiddEval3/testH/Bicycle2/im0.png\"\n",
    "    #\"/data/stereo_data/middlebury/MiddEval3/testF/Classroom2/im0.png\"\n",
    "    img_path_right= \"\"\n",
    "    # \"/data/stereo_data/middlebury/MiddEval3/testH/Bicycle2/im1.png\"\n",
    "    #\"/data/stereo_data/middlebury/MiddEval3/testF/Classroom2/im1.png\"\n",
    "\n",
    "    # pre-process;\n",
    "    left_img = Image.open(img_path_left).convert('RGB')\n",
    "    left_img = left_img.resize((448,224))\n",
    "\n",
    "    right_img = Image.open(img_path_right).convert('RGB')\n",
    "    right_img = right_img.resize((448,224))\n",
    "\n",
    "    assert np.shape(left_img) == (224, 448, 3)\n",
    "    assert np.shape(right_img) == (224, 448, 3)\n",
    "\n",
    "    # normalize by ImageNet mean and std\n",
    "    #img = img - imagenet_mean\n",
    "    #img = img / imagenet_std\n",
    "    totensor = transforms.ToTensor()\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = [5, 5]\n",
    "    show_image(totensor(left_img).permute(1,2,0), 'left image')\n",
    "    show_image(totensor(right_img).permute(1,2,0), 'right image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model for reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if LOAD_IMG== \"dataloader\":\n",
    "    run_one_image(left_img, right_img, model)\n",
    "    \n",
    "else:\n",
    "    totensor = transforms.ToTensor()\n",
    "    torch.manual_seed(2) # <<< random seed for random masking.\n",
    "    left_img_t = totensor(left_img)\n",
    "    right_img_t = totensor(right_img)\n",
    "    run_one_image(left_img_t, right_img_t, model, 0.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c951534619e2844a34909bcab2ba7fe4df669e60c5a6c2262e70445536120363"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
