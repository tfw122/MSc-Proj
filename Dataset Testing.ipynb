{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a59fd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      "Importing\n"
     ]
    }
   ],
   "source": [
    "#from src.utils.builder import build_trainer, build_config\n",
    "print(\"Hi\") \n",
    "from src.utils.utils import *\n",
    "from src.utils.builder import *\n",
    "from src.utils.fileio import *\n",
    "from src.common.registry import registry\n",
    "from arguments import args\n",
    "from time import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm \n",
    "\n",
    "print(\"Importing\")\n",
    "\n",
    "# --------------- Import Dataset Builder function -----------------\n",
    "from src.datasets.dataset_zoo.stereo_vision_mim.stereo_datasets_mim.cityscapes_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_mim.stereo_datasets_mim.kitti_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_mim.stereo_datasets_mim.falling_things_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_mim.stereo_datasets_mim.sceneflow_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_mim.stereo_datasets_mim.sintel_stereo_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_mim.stereo_datasets_mim.tartan_air_dataset import *\n",
    "\n",
    "from src.datasets.dataset_zoo.stereo_vision_downstream.stereo_datasets_downstream.kitti_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_downstream.stereo_datasets_downstream.falling_things_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_downstream.stereo_datasets_downstream.sceneflow_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_downstream.stereo_datasets_downstream.sintel_stereo_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_downstream.stereo_datasets_downstream.tartan_air_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_downstream.stereo_datasets_downstream.eth3d_dataset import *\n",
    "from src.datasets.dataset_zoo.stereo_vision_downstream.stereo_datasets_downstream.middlebury_dataset import *\n",
    "\n",
    "from src.datasets.dataset_zoo.stereo_vision_mim.stereo_vision_mim_builder import StereoVisionMaskedImageModellingDatasetModule\n",
    "\n",
    "global TESTING \n",
    "TESTING = \"downstream\" # either 'downstream' or 'mim'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da5b62f",
   "metadata": {},
   "source": [
    "# Defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a31352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "    \n",
    "def get_transforms(config, split):\n",
    "    transforms_name = config.dataset_config.preprocess.name\n",
    "    data_transform_cls = registry.get_preprocessor_class(transforms_name)\n",
    "    data_transforms_obj = data_transform_cls(config, split)\n",
    "    return data_transforms_obj\n",
    "\n",
    "def ToArray(img_t):\n",
    "    img = img_t.detach().to(\"cpu\").numpy()\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900aeffc",
   "metadata": {},
   "source": [
    "# Standard input arguments (config paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "553f538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    default_config_path= './configs/default.yaml'\n",
    "    model_config_path='./configs/models/masked_image.yaml'\n",
    "    if TESTING=='downstream':\n",
    "        dataset_config_path='./configs/datasets/stereo_downstream.yaml'\n",
    "    else:\n",
    "        dataset_config_path='./configs/datasets/stereo_mim.yaml'\n",
    "    user_config_path='./configs/user/sample.yaml'\n",
    "    local_rank=None\n",
    "    opts=None\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8d7c7",
   "metadata": {},
   "source": [
    "# File IO functions initialised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c9bd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42424242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS VERSION OF THE CODE SUPPORTS LOCAL FILE I/O ONLY! FOR CLOUD CLIENTS CONTACT THE MAINTAINER\n",
      "output dir for saving files: \"../data/tiawarner/downstream4/mae_stereo_mim_perceptual/230827-171017/train_outputs\" created!\n",
      "stereo_downstream\n"
     ]
    }
   ],
   "source": [
    "# --------------- configure initialisers -----------------\n",
    "setup_imports()\n",
    "\n",
    "config = build_config(args)\n",
    "\n",
    "# setup aws env variables:\n",
    "fileio_client = FileIOClient(config)\n",
    "\n",
    "# check correct config loaded;\n",
    "print(config.dataset_config.dataset_name)\n",
    "#print(config.dataset_config.usable_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed427d",
   "metadata": {},
   "source": [
    "# Define dataset transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93fb2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = get_transforms(config, 'train')\n",
    "test_transforms = get_transforms(config, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7840c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_parameters(config):\n",
    "    dataset_config = config.dataset_config\n",
    "    preprocess_config = dataset_config.preprocess.vision_transforms.params\n",
    "    aug_params = {'crop_size': preprocess_config.Resize.size, 'min_scale': preprocess_config.spatial_scale[0], 'max_scale': preprocess_config.spatial_scale[1], 'do_flip': preprocess_config.do_flip, 'yjitter': not preprocess_config.noyjitter}\n",
    "    \n",
    "    if hasattr(preprocess_config, \"saturation_range\") and preprocess_config.saturation_range is not None:\n",
    "        aug_params[\"saturation_range\"] = tuple(preprocess_config.saturation_range)\n",
    "    \n",
    "    if hasattr(preprocess_config, \"img_gamma\") and preprocess_config.img_gamma is not None:\n",
    "        aug_params[\"gamma\"] = preprocess_config.img_gamma\n",
    "    \n",
    "    if hasattr(preprocess_config, \"do_flip\") and preprocess_config.do_flip is not None:\n",
    "        aug_params[\"do_flip\"] = preprocess_config.do_flip\n",
    "    return aug_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83204fe",
   "metadata": {},
   "source": [
    "# Define dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "882d4142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. training # samples: 0\n",
      "no. val # samples: 0\n"
     ]
    }
   ],
   "source": [
    "# -------------- Initialise dataset -----------------\n",
    "if TESTING=='downstream':\n",
    "    aug_params = augmentation_parameters(config)\n",
    "    train_dataset = SceneFlowDatasetsClean(config = config, aug_params = aug_params, split='train')\n",
    "    val_dataset = SceneFlowDatasetsClean(config = config, aug_params = aug_params, split='val')\n",
    "else:\n",
    "    train_dataset = SceneFlowLoader(config, 'train', train_transforms)\n",
    "    val_dataset = SceneFlowLoader(config, 'val', train_transforms)\n",
    "\n",
    "# iterate over the dataset\n",
    "print('no. training # samples: {}'.format(len(train_dataset)))\n",
    "print('no. val # samples: {}'.format(len(val_dataset)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cd79e9b",
   "metadata": {},
   "source": [
    "# Test file name compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aa6db97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "if TESTING=='downstream':\n",
    "    for i in tqdm(range(len(train_dataset))):\n",
    "        #if i > 0:\n",
    "        #    break\n",
    "        #else:\n",
    "        # left_image, right_image = dataset[i]\n",
    "        sample = train_dataset[i]\n",
    "        image_disp_path_list, left_img, right_img, disp, valid = sample\n",
    "        left_idx = image_disp_path_list[0][78:-4]\n",
    "        right_idx = image_disp_path_list[1][79:-4]\n",
    "        disp_idx = image_disp_path_list[2][71:-4]\n",
    "\n",
    "        if disp_idx != right_idx or disp_idx != left_idx or right_idx!=left_idx:\n",
    "            print('the following, left: {}, right: {} and disp: {} idx do not match'.format(left_dix, right_idx, disp_idx))\n",
    "            print(image_disp_path_list)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114de23",
   "metadata": {},
   "source": [
    "# Visualise samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bef8cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- Iterate over a few samples ------------\n",
    "if TESTING=='downstream':\n",
    "    for i in range(len(train_dataset)):\n",
    "        if i > 3:\n",
    "            break\n",
    "        else:\n",
    "            # left_image, right_image = dataset[i]\n",
    "            sample = train_dataset[i]\n",
    "            # base stereo dataset returns;\n",
    "            # self.image_list[index] + [self.disparity_list[index]], img1, img2, flow, valid.float()\n",
    "            image_disp_path_list, left_img, right_img, disp, valid = sample\n",
    "            print(image_disp_path_list)\n",
    "            \n",
    "            left_image= ToArray(left_img)\n",
    "            right_image= ToArray(right_img)\n",
    "            disp_map = ToArray(disp)\n",
    "            valid_map = ToArray(valid.unsqueeze(0))\n",
    "            \n",
    "            fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4)\n",
    "            fig.suptitle('Downstream Stereo Images')\n",
    "            ax1.imshow(left_image.astype(np.uint8))\n",
    "            ax2.imshow(right_image.astype(np.uint8))\n",
    "            ax3.imshow(disp_map, cmap='jet')\n",
    "            ax4.imshow(valid_map)\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "\n",
    "else:\n",
    "    for i in range(len(train_dataset)):\n",
    "        if i > 3:\n",
    "            break\n",
    "        else:\n",
    "            # left_image, right_image = dataset[i]\n",
    "            sample = train_dataset[i]\n",
    "            left_image, right_image = sample['left_image'], sample['right_image']\n",
    "            \n",
    "            print(i, type(sample))\n",
    "            print(left_image.size(), right_image.size())\n",
    "            \n",
    "            left_img= ToArray(left_image)\n",
    "            right_img= ToArray(right_image)\n",
    "\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "            fig.suptitle('MIM Stereo Images')\n",
    "            ax1.imshow(left_img)\n",
    "            ax2.imshow(right_img)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2fdf17f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'left_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mleft_img\u001b[49m\u001b[38;5;241m.\u001b[39mmax(), left_img\u001b[38;5;241m.\u001b[39mmin())\n\u001b[1;32m      3\u001b[0m totensor \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[1;32m      5\u001b[0m left_imgx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/stereo_data/fallingthings/fat/single/002_master_chef_can_16k/kitchen_0/000003.left.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'left_img' is not defined"
     ]
    }
   ],
   "source": [
    "print(left_img.max(), left_img.min())\n",
    "\n",
    "totensor = transforms.ToTensor()\n",
    "\n",
    "left_imgx = np.asarray(Image.open('/data/stereo_data/fallingthings/fat/single/002_master_chef_can_16k/kitchen_0/000003.left.jpg').convert('RGB'))\n",
    "right_imgx = np.asarray(Image.open('/data/stereo_data/fallingthings/fat/single/002_master_chef_can_16k/kitchen_0/000003.right.jpg').convert('RGB'))\n",
    "\n",
    "left_img_t = torch.from_numpy(left_imgx).permute(2, 0, 1).float()\n",
    "right_img_t = torch.from_numpy(right_imgx).permute(2, 0, 1).float()\n",
    "\n",
    "print(left_img_t.max(), left_img.min())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "c951534619e2844a34909bcab2ba7fe4df669e60c5a6c2262e70445536120363"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
